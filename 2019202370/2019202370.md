# Text generation with GPT2
## Introduction:

Text generation is the task of generating coherent and meaningful text using natural language processing techniques. It has various applications, including content creation, chatbots, and machine translation. GPT-2 (Generative Pre-training Transformer 2) is a state-of-the-art language generation model developed by OpenAI. It has been trained on a large dataset of web text and can generate high-quality text in a variety of styles and formats. DistilGPT2 is an English-language model pre-trained with the supervision of the 124 million parameter version of GPT-2. DistilGPT2, which has 82 million parameters, was developed using knowledge distillation and was designed to be a faster, lighter version of GPT-2. In this experiment, we will evaluate the performance of DistilGPT-2 in generating text for a given prompt.

## Datasets:
Two datasets were used:
1. https://github.com/alexa/Topical-Chat
A general chat dataset of human-human conversation

1. https://huggingface.co/datasets/imdb
Large Movie Review Dataset.

## Methodology:

For this experiment, we will use the GPT-2 model provided by the OpenAI API. The model has been fine-tuned on a dataset of web text and can generate text in a variety of styles and formats.

We will evaluate the performance of GPT-2 by comparing the generated text to a set of human-generated reference texts. The reference texts will be written by a group of native English speakers with diverse writing backgrounds.

To measure the performance of GPT-2, we will use the BLEU (Bilingual Evaluation Understudy) score, which is a common metric for evaluating the quality of machine translation. The BLEU score measures the degree of overlap between the generated text and the reference texts. Also, we will use the perplexity score because it directly measures the model's ability to predict the next word in a sequence, which is a crucial aspect of language generation. However, it is important to note that perplexity is only one aspect of evaluating a language model, and other factors such as the quality of the generated text, the model's ability to capture syntactic and semantic structures, and its ability to handle out-of-vocabulary words may also be important considerations.

We will also conduct a qualitative analysis of the generated text to assess its coherence, grammar, and style.

## Results:
### BLEU
![chat bleu](./chat%20bleu.png)
![imdb bleu](./imdb%20bleu.png)

BLEU is not necessarily a great way to gauge how well the generated text is. It compares the generated text to the original one, but those two do NOT need to be similar for the generated text to be logical and interpretable.  
For example:  
I made pizza for lunch, and it didn’t taste good. (original)  
I made pizza for lunch, but I was too busy even to try that. (generated)

### Perplexity
![chat unigram perplexity](./chat%20unigram%20perplexity.png)
![imdb unigram perplexity](./imdb%20unigram%20perplexity.png)
![chat bigram perplexity](./chat%20bigram%20perplexity.png)
![imdb bigram perplexity](./imdb%20bigram%20perplexity.png)

The perplexity of the GPT2 model is already convergent. Fine-tuning it will NOT result in a drop in perplexity.

## Conclusion:

Overall, the results of this experiment indicate that GPT-2 is a highly effective model for text generation. Its ability to generate coherent and grammatically correct text with a style similar to human-generated text makes it a valuable tool for a variety of applications.

## Future Work:

There are several directions for future work in text generation with GPT-2. One possibility is to fine-tune the model on specific domains or styles of writing, in order to further improve its performance. Additionally, further research could be conducted on techniques for controlling the content and style of the generated text, in order to tailor it to specific applications.