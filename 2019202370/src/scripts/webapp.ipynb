{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5145/409314587.py:3: UserWarning: \n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n",
      "  import dash_core_components as dcc\n",
      "/tmp/ipykernel_5145/409314587.py:4: UserWarning: \n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "  import dash_html_components as html\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "device=torch.device('cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model_chat = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model_imdb = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
    "model_chat.load_state_dict(torch.load(\"./reddit_chat_text_gen_epoch20.pt\"))\n",
    "model_chat.load_state_dict(torch.load(\"./IMDB_text_gen_epoch20.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://0.0.0.0:8050/\n",
      "\n",
      "Dash is running on http://0.0.0.0:8050/\n",
      "\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "#START OF THE ACTUAL APP INTERFACE\n",
    "app = dash.Dash()\n",
    "\n",
    "#Create the layout of the app\n",
    "app.layout = html.Div([\n",
    "    \n",
    " html.H1(children='TEXT GENERATOR', style={'textAlign': 'center'\n",
    "        }),\n",
    "    \n",
    "#Add dropdown menu for type of\n",
    "dcc.Dropdown(id='models',\n",
    "    options=[\n",
    "        {'label': 'original', 'value': 'original'},\n",
    "        {'label': 'chat', 'value': 'chat'},\n",
    "        {'label': 'IMDB', 'value': 'IMDB'}\n",
    "    ],\n",
    "    value='original'),\n",
    "\n",
    "#Add place where text would be inserted    \n",
    "  dcc.Textarea(\n",
    "        id='textarea-state-example',\n",
    "        value='',\n",
    "        style={'width': '99%', 'height': 200, 'font-size': 'large','margin-top': '10px','margin-bottom': '10px'},\n",
    "    ),\n",
    "    html.Button('Submit', id='textarea-state-example-button', n_clicks=0, \n",
    "               style={'font-size': '12px', 'width': '140px', 'display': 'inline-block','margin-top': '10px', 'margin-bottom': '10px', 'margin-right': '5px', 'height':'25px'}),\n",
    "    \n",
    "    html.Div(id='textarea-state-example-output', style={'whiteSpace': 'pre-line', 'position':'absolute', \n",
    "                        'margin-left': 'auto', 'margin-right': 'auto', 'top':'400px',\n",
    "                        'font-size': 'large','height': '200px', 'width': 'auto'})\n",
    "    \n",
    "])  \n",
    "\n",
    "@app.callback(\n",
    "    Output('textarea-state-example-output', 'children'),\n",
    "    [Input('textarea-state-example-button', 'n_clicks'), Input('models', 'value')],\n",
    "    State('textarea-state-example', 'value')\n",
    ")\n",
    "\n",
    "def update_output(n_clicks, value, value2):\n",
    "    \n",
    "    if n_clicks > 0:\n",
    "        if value == 'original':\n",
    "            generated = generate(model.to('cpu'), tokenizer, value2, entry_count=1)\n",
    "            \n",
    "            #Clean the output\n",
    "            generated2 = ' '.join(generated)\n",
    "            to_remove = generated2.split('.')[-1]\n",
    "            \n",
    "            my_text = generated2.replace(to_remove,'')\n",
    "        \n",
    "        if value == 'chat':\n",
    "            generated = generate(model_chat.to('cpu'), tokenizer, value2, entry_count=1)\n",
    "            \n",
    "            #Clean the output\n",
    "            generated2 = ' '.join(generated)\n",
    "            to_remove = generated2.split('.')[-1]\n",
    "            \n",
    "            my_text = generated2.replace(to_remove,'')\n",
    "        if value == 'IMDB':\n",
    "            generated = generate(model_imdb.to('cpu'), tokenizer, value2, entry_count=1)\n",
    "            \n",
    "            #Clean the output\n",
    "            generated2 = ' '.join(generated)\n",
    "            to_remove = generated2.split('.')[-1]\n",
    "            \n",
    "            my_text = generated2.replace(to_remove,'')              \n",
    "        \n",
    "        return my_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server(debug=True, host=\"0.0.0.0\", use_reloader=False, port=8050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
