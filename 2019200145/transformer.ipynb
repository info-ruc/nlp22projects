{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任务描述：语言模型的任务就是给定一个文本序列，然后基于此序列去预测下一个词或一个序列。\n",
    "\n",
    "\n",
    "数据集：使用的数据集是 torchtext 中的 Wikitext-2 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-29071640f3a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "torchtext.data.Field 声明处理数据的方式\n",
    "参数说明：\n",
    "    tokenize 分词处理\n",
    "    init_token 定义开始字符\n",
    "    eos_token 定义结束字符\n",
    "    lower 小写化处理\n",
    "'''\n",
    "# 声明处理方式，主要包括分词和小写化处理\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT, root='./datasets', train='wiki.train.tokens', validation='wiki.valid.tokens', test='wiki.test.tokens')\n",
    "\n",
    "# 统计数据\n",
    "print('train_txt tokens: %d' % len(train_txt.examples[0].text))\n",
    "print('val_txt tokens:   %d' % len(val_txt.examples[0].text))\n",
    "print('test_txt tokens:  %d' % len(test_txt.examples[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依据训练集构建词典\n",
    "TEXT.build_vocab(train_txt)\n",
    "\n",
    "# 查看词典\n",
    "length = len(TEXT.vocab)\n",
    "print('词表大小: %d' % length)\n",
    "TEXT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    '''\n",
    "    将数据划分为用于训练的批次\n",
    "    '''\n",
    "    # 将文本形式的数据用 token 的相应索引来表示\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # 获取总的批次数目\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # 去除剩余的部分，比如总长度为12，而批次大小为5，那么剩余的2个 token 将不会被包括在内\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # 根据批次大小，划分数据集\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35 # 句子长度\n",
    "def get_batch(source, i):\n",
    "    '''\n",
    "    把数据进一步切分成长度为35的序列，最后返回的 data:[35, batch_size] ,每一列表示一个连续的序列\n",
    "    '''\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = get_batch(train_data, 0)\n",
    "print(data.size())\n",
    "print(targets.size()) # target 表示待预测的下一个正确的词，用于计算模型损失，进而更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ''\n",
    "for id in data[:, 0]:\n",
    "    src = src + (' %s' % TEXT.vocab.itos[id])\n",
    "    \n",
    "src.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = ''\n",
    "for id in targets[0::20]:\n",
    "    tgt = tgt + (' %s' % TEXT.vocab.itos[id])\n",
    "    \n",
    "tgt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    '''\n",
    "    给原始序列添加位置编码\n",
    "    '''\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 首先初始化为0\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # sine 和 cosine 来生成位置信息\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 词经过嵌入层后，再加上位置信息\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "     def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout) # 位置编码\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout) # EncoderLayer\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # Encoder\n",
    "        self.encoder = nn.Embedding(ntoken, ninp) # 嵌入层\n",
    "        self.ninp = ninp # 模型维度\n",
    "        \n",
    "        # decoder 用于将隐藏层的表示转化成词表中 token 的概率分布\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # 生成 mask ，保证模型只能看到当前位置之前的信息\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src) # 位置编码\n",
    "        output = self.transformer_encoder(src, mask=self.src_mask, src_key_padding_mask=None)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab.stoi) # 词表大小\n",
    "emsize = 200 # 嵌入层维度\n",
    "nhid = 200 # nn.TransformerEncoder 中前馈神经网络的维度\n",
    "nlayers = 2 # 编码器中 nn.TransformerEncoderLayer 层数\n",
    "nhead = 2 # 多头注意力机制中“头”的数目\n",
    "dropout = 0.2 # dropout\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xavier_normal_初始化参数\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_normal_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率\n",
    "lr = 2.0\n",
    "# 随机梯度下降\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# 动态调整学习率\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train() # 训练模式，更新模型参数\n",
    "    total_loss = 0.\n",
    "    start_time = time.time() # 用于记录模型的训练时长\n",
    "    ntokens = len(TEXT.vocab.stoi) # 词表大小\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        # 获取批次数据\n",
    "        data, targets = get_batch(train_data, i) \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # 计算损失\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        # 计算梯度\n",
    "        loss.backward()\n",
    "        # 梯度裁剪，防止梯度消失/爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        # 优化参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印训练记录\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # 评估模式，不更新模型参数，仅评估模型当前的表现\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi) # 词表大小\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # 共训练3个epoch\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1): \n",
    "# for epoch in range(1, 2): # Kagging test\n",
    "    epoch_start_time = time.time()\n",
    "    # 训练过程\n",
    "    train()\n",
    "    # 验证过程\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    # 打印验证结果\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    # 记录最佳模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "    \n",
    "    # 调整学习率\n",
    "    scheduler.step()\n",
    "    # # 保存模型\n",
    "# if not os.path.exists('datasets/models'):\n",
    "#     os.makedirs('datasets/models')\n",
    "# torch.save({'state_dict': model.state_dict()}, 'datasets/models/best_model.pth.tar')\n",
    "\n",
    "# 保存模型\n",
    "if not os.path.exists('temp/models'):\n",
    "    os.makedirs('temp/models')\n",
    "torch.save({'state_dict': model.state_dict()}, 'temp/models/best_model.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算交叉熵损失\n",
    "test_loss = evaluate(best_model, test_data)\n",
    "\n",
    "# 计算困惑度\n",
    "ppl = math.exp(test_loss)\n",
    "print('=' * 40)\n",
    "print('| End of training | test ppl {:8.2f}'.format(ppl))\n",
    "print('=' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(len(TEXT.vocab.stoi), ninp=200, nhead=2, nhid=200, nlayers=2, dropout=0.2).to(device)\n",
    "# 模型加载训练好的参数\n",
    "# checkpoint = torch.load('datasets/models/best_model.pth.tar')\n",
    "checkpoint = torch.load('temp/models/best_model.pth.tar')\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已知序列\n",
    "history = 'This was followed by a starring role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = []\n",
    "for w in history.split():\n",
    "    h.append([TEXT.vocab.stoi[w]])\n",
    "\n",
    "while(True):\n",
    "    # 把列表转化成 tensor ，然后计算模型输出\n",
    "    output = model(torch.tensor(h).to(device))\n",
    "    # 获取概率最大的5个单词的 id\n",
    "    idxs = output[-1].argsort(descending=True).view(-1)[:10]\n",
    "    # 随机选择其中一个\n",
    "    r = random.randint(0, 10)\n",
    "    h.append([r])\n",
    "    # 句子结束\n",
    "    if TEXT.vocab.itos[r] == '.' or TEXT.vocab.itos[r] == '<eos>':\n",
    "        break\n",
    "\n",
    "# 将下标转化成句子\n",
    "sent = ''\n",
    "for w in h:\n",
    "    sent += TEXT.vocab.itos[w[0]] + ' '\n",
    "\n",
    "# out_path = './tmp/hypotheses.txt'\n",
    "out_path = './temp/hypotheses.txt'\n",
    "# out_path = './submit/hypotheses.txt'\n",
    "with open(out_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('history: ' + history + '\\n')\n",
    "    f.write('hypotheses: ' + sent + '\\n')\n",
    "    \n",
    "print(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
